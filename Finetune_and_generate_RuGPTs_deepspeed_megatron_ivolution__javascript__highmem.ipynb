{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Finetune_and_generate_RuGPTs_deepspeed_megatron_ivolution__javascript__highmem.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VCapfDfeBq0x",
        "KRlEwlPdE0L8"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StrangeTcy/ru-gpts/blob/master/Finetune_and_generate_RuGPTs_deepspeed_megatron_ivolution__javascript__highmem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HYpFZk7Ds5C",
        "outputId": "3cc15f3f-f015-441d-b8b9-fa709f634de9"
      },
      "source": [
        "# Mount our drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubAw6JWPwUpV",
        "outputId": "4d91511f-d882-4aa1-d65d-7a7dad408139"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Apr  7 05:29:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XYncS485bN6j"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRzAXrPVzsHX"
      },
      "source": [
        "# Finetune RuGPTs in megatron and deepspeed\n",
        "How to finetune RuGPTs models with megatron and deepspeed. Example for RuGPT3Small. Note for other models it will take more GPU memory.\n",
        "\n",
        "This notebook is valid for all RuGPTs models except RuGPT3XL.\n",
        "## Install env"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hu1OzWZ6zqQv",
        "outputId": "bf8c6c62-5550-46c6-a33c-8de61813e8b4"
      },
      "source": [
        "# !pip3 install transformers==3.5.0\n",
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 22.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=9535893866ecc76fe0d569fb7e9b64c88353e21cc9ffea3845f0278a285b4b4c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ozJOYbK-11pk",
        "outputId": "0bef045d-79ed-4908-9828-4cf374988f64"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M46Pk6DJ19Jk"
      },
      "source": [
        "# We might install apex later on, but for now we don't need or use it\n",
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gE7xBM_z-uW"
      },
      "source": [
        "# Git clone my clone of sberbank's ru-gpts\n",
        "!git clone  https://github.com/StrangeTcy/ru-gpts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bVWryahFmtx"
      },
      "source": [
        "# Install deepspeed which we'll probably need later on\n",
        "!pip install deepspeed==0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1BvuoGuESqh"
      },
      "source": [
        "# Install packages we'll need if we use sparsify (we mgiht?)\n",
        "!apt-get install llvm-9-dev\n",
        "!pip install cpufeature\n",
        "!pip install triton==0.2.3\n",
        "!DS_BUILD_CPU_ADAM=1 DS_BUILD_SPARSE_ATTN=1 pip install deepspeed==0.3.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wawWCxCvEfWb"
      },
      "source": [
        "# Is deepspeed working ok?\n",
        "!ds_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8di4sCoS0Pyw"
      },
      "source": [
        "## Download files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96qG_A1n0CiF"
      },
      "source": [
        "# !wget -O train.txt https://www.dropbox.com/s/oa3v9c7g9bp40xw/train.txt?dl=0\n",
        "# !wget -O valid.txt https://www.dropbox.com/s/mworl3ld6r3bg62/valid.txt?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iddnTtsYBzv4"
      },
      "source": [
        "Convert our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE-wUifeFV5E"
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1u50bbNb5p9cjlx99YuQm1m9yTOgGDiNO\n",
        " \n",
        "\n",
        "with open(\"/content/drive/MyDrive/ivolution_dataset/js_script_prechunks.json\", \"r\") as ptj:\n",
        "  sources = ptj.readlines()\n",
        "\n",
        "# with open(\"/content/js_script_prechunks.json\", \"r\") as ptj:\n",
        "#   sources = ptj.readlines()\n",
        "\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "chunks = defaultdict(list)\n",
        "num = 0\n",
        "\n",
        "for l in tqdm(sources[:1000]):\n",
        "  \n",
        "  \n",
        "  if '+++++\\n' in l:\n",
        "    num +=1\n",
        "  else:\n",
        "    chunks[num].append(l)\n",
        "  \n",
        "  first_line = chunks.get(num)\n",
        "  if first_line is not None:\n",
        "    l_splits = first_line[0].split(\" \") \n",
        "    # for s in l_splits:\n",
        "    #   print (f\"\\t\\t========{s}\")\n",
        "  \n",
        "  # with open(\"/content/trial.txt\", \"a+\") as f:\n",
        "  with open(\"/content/huge_dataset.txt\", \"a+\") as f:\n",
        "    for j in range(len(chunks)):\n",
        "      for i in range(max(len(chunks[j]),3)):\n",
        "        prompt = str(''.join(chunks[j][:i]))\n",
        "        # a reasonable chunk of code for completeion  \n",
        "        lng = 5\n",
        "        completion = str(''.join(chunks[j][i:i+lng]))\n",
        "        if not (prompt == '' or completion == ''):\n",
        "          f.write(f\"<s>prompt: {prompt}\\n completion:{completion}</s>\\n\")\n",
        "      first_line = chunks.get(j)\n",
        "      if first_line is not None:\n",
        "        l_splits = first_line[0].split(\" \") \n",
        "        for s in range(len(l_splits)):\n",
        "          # print (f\"\\t\\t========{l_splits[s]}\")\n",
        "          prompt = str(\" \".join(l_splits[:s]))\n",
        "          completion =  str(\" \".join(l_splits[s:]))\n",
        "          if not (prompt == '' or completion == ''):\n",
        "            f.write(f\"<s>Prompt: {prompt}\\nCompletion: {completion}</s>\\n\")\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okPKzLb_zqOW"
      },
      "source": [
        "# Get our dataset\n",
        "# !gdown https://drive.google.com/file/d/1-0K7OdvgX9AuPbJakCaAWgpL5qCSyuB9\n",
        "# !cp -v /content/drive/Shareddrives/Machine_/ivolution_dataset/huge_dataset.txt /content/trial.txt\n",
        "# !wc -l /content/drive/Shareddrives/Machine_/ivolution_dataset/huge_dataset.txt\n",
        "# !wc -l /content/drive/MyDrive/huge_dataset.txt\n",
        "# !wc -l /content/huge_dataset.txt     \n",
        "print(int(8927077*0.9), int(8927077*0.1))\n",
        "# input (\"Use that?\")\n",
        "\n",
        "# Train/test split\n",
        "380253500-342228150-38025350\n",
        "!head -n 8034369 /content/huge_dataset.txt > /content/train.txt\n",
        "!tail -892707 /content/huge_dataset.txt > /content/valid.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGnuMuHNB7ne"
      },
      "source": [
        "Train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFSV7gP4B-uD"
      },
      "source": [
        "# !pwd\n",
        "# %cd /content/auto_coding\n",
        "# !python3 train.py --model_select gpt2_large --with_wandb True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqW38Hni64xH"
      },
      "source": [
        "## Prepare data for parallel\n",
        "We use custom implementation of distributed dataset. For training and evaluating we should specify file `file.list` with list of paths to txt files. All files from `file.list` will be splitted between aviable GPUs. The logic of splitting is described by the following code:\n",
        "\n",
        "```python\n",
        "shard_size = len(files) // world_size\n",
        "shard_start = rank * shard_size\n",
        "shard_end = (rank + 1) * shard_size\n",
        "files = files[shard_start:shard_end]\n",
        "```\n",
        "\n",
        "For more details please see full code of dataset: `src.dataset_rugpt3.RuGpt3TextDataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtItuLGA38db"
      },
      "source": [
        "!echo /content/train.txt > /content/train.list\n",
        "!echo /content/valid.txt > /content/valid.list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EF0JepF0S41"
      },
      "source": [
        "## Train\n",
        "Load model from Huggingface and finetune on essays.\n",
        "\n",
        "This will take arount ten minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC_NcmEK1nf2"
      },
      "source": [
        "!mkdir log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2YAl52J25M5"
      },
      "source": [
        "%tensorboard --logdir log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s79PfuyQWqXg"
      },
      "source": [
        "%cd /content/ru-gpts\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHluAlFh0SJo"
      },
      "source": [
        "# > padded vocab (size: 50257) with 7 dummy tokens (new size: 50264)\n",
        "\n",
        "# %tensorboard --logdir log\n",
        "%cd /content\n",
        "!export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "!USE_DEEPSPEED=1 python -m torch.distributed.launch --nproc_per_node 1 ru-gpts/pretrain_gpt3.py \\\n",
        "  --train-data-path \"train.list\" \\\n",
        "  --test-data-path \"valid.list\" \\\n",
        "  --max-files-per-process 100 \\\n",
        "  --logging-dir=\"log\" \\\n",
        "  --save /content/drive/MyDrive/ivolution_gpt/model \\\n",
        "  --load-huggingface sberbank-ai/rugpt3large_based_on_gpt2 \\\n",
        "  --save-interval 1000 \\\n",
        "  --log-interval 100 \\\n",
        "  --eval-interval 1000 \\\n",
        "  --eval-iters 100 \\\n",
        "  --model-parallel-size 1 \\\n",
        "  --num-layers 24 \\\n",
        "  --hidden-size 1536 \\\n",
        "  --num-attention-heads 16 \\\n",
        "  --batch-size 1 \\\n",
        "  --seq-length 2048 \\\n",
        "  --max-position-embeddings 2048 \\\n",
        "  --train-iters 2000 \\\n",
        "  --resume-dataloader \\\n",
        "  --distributed-backend \"nccl\" \\\n",
        "  --lr 0.00015 \\\n",
        "  --lr-decay-style \"cosine\" \\\n",
        "  --lr-decay-iters 3200 \\\n",
        "  --clip-grad 0.5 \\\n",
        "  --warmup .004 \\\n",
        "  --fp16 \\\n",
        "  --checkpoint-activations \\\n",
        "  --deepspeed-activation-checkpointing \\\n",
        "  --deepspeed \\\n",
        "  --deepspeed_config ru-gpts/src/deepspeed_config/gpt3_large_2048.json &>> /content/drive/MyDrive/gpt3_log6.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtWIBU_PRZTg"
      },
      "source": [
        "# %tensorboard --logdir log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALvcD5SE8RtP"
      },
      "source": [
        "At the end of training output should be something like this:\n",
        "\n",
        "\"-----------------------------------------------------------------------------------------\n",
        "\n",
        " validation loss at the end of training for test data | LM loss: 3.0002 | LM PPL: 20.090\n",
        "\n",
        "-----------------------------------------------------------------------------------------\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HmKilrb8lQm"
      },
      "source": [
        "## Generate\n",
        "\n",
        "Load pretrained model from dir and generate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAH-WpCG8lmG"
      },
      "source": [
        "# !export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "# !python ru-gpts/generate_samples.py \\\n",
        "#   --load model/ \\\n",
        "#   --model-parallel-size 1 \\\n",
        "#   --num-layers 12 \\\n",
        "#   --hidden-size 768 \\\n",
        "#   --num-attention-heads 12 \\\n",
        "#   --batch-size 1 \\\n",
        "#   --seq-length 500 \\\n",
        "#   --max-position-embeddings 2048 \\\n",
        "#   --distributed-backend \"nccl\" \\\n",
        "#   --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "#   --no-load-optim\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCapfDfeBq0x"
      },
      "source": [
        "### Convert checkpoint to Huggingface format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnhIyqd-Eeo"
      },
      "source": [
        "# !export PYTHONPATH=${PYTHONPATH}:${HOME}/ru-gpts\n",
        "\n",
        "# !python ru-gpts/convert2huggingface.py \\\n",
        "#   --load model/ \\\n",
        "#   --model-parallel-size 1 \\\n",
        "#   --num-layers 12 \\\n",
        "#   --hidden-size 768 \\\n",
        "#   --num-attention-heads 12 \\\n",
        "#   --max-position-embeddings 2048 \\\n",
        "#   --tokenizer-path sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "#   --no-load-optim \\\n",
        "#   --export-huggingface model_hf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRlEwlPdE0L8"
      },
      "source": [
        "#### Test load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U81i24aEEm0"
      },
      "source": [
        "# from transformers import GPT2LMHeadModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBRatZnJEcCX"
      },
      "source": [
        "# model = GPT2LMHeadModel.from_pretrained(\"model_hf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BOvY12Oertm"
      },
      "source": [
        "# !mkdir /content/drive/MyDrive/ivolution_gpt\n",
        "# !cp -rv /content/model /content/drive/MyDrive/ivolution_gpt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}